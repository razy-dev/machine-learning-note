include::adoc/_toc.adoc[]

= Artificial Neural Network

== Overview

손실 함수(Loss Function)::
최적화(Optimizer)::
역전파(Back Propagation)::

//==

== 손실 함수(Loss Function)

=== 제곱오차(Square Error, SE)

stem:[SE = (y - \hat{y})^2]

=== 오차제곱합(Sum of Squares for Error, SSE)

기본식::
stem:[SSE = \sum_{k}(y_k - \hat{y_k})^2]

신경망에서는::
stem:[E = \frac{1}{2}\sum_{k}(y_k - \hat{y_k})^2 ] +
+
오류를 최소화시키기 위해 stem:[\frac{1}{2}] 을 곱해준다.

=== 평균제곱오차(Mean Square Error, MSE)

stem:[\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2]

=== 평균제곱근오차(Root Mean Square Error, RMSE)

stem:[RMSE = \sqrt{\frac{1}{n}\sum_{k}^{n}(y_k - \hat{y_k})^2}]

=== 교차 엔트로피 오차(Cross Entropy Error, CEE)

=== 이진 교차 엔트로피 오차(Binary Cross Entropy Error)

=== 범주형 교차 엔트로피 오차(Categorical Cross Entropy Error, CCEE)

== 최적화(Optimizer)

stem:[SE = (y - \hat{y})^2]

stem:[\hat{y} = wx + b]

stem:[(y - \hat{y})^2] +
stem:[= (y - (wx + b))^2] +
stem:[= y^2 -2y(wx + b) + (wx + b)^2] +
stem:[= w^2x^2 + 2wxb + b^2 - 2wxy - 2yb + y^2]

=== 경사하강법(Gradient Descent)

stem:[x_{i+1} = x_i - \eta\bigtriangledown f(x_i)]::
stem:[\eta]:::: 학습률(Learning Rate)이라 하며, 한 번의 학습에서 얼마나 이동할지를 정한다.

stem:[\bigtriangledown f(x_i)]::::
스칼라 함수 stem:[f(x)]의 기울기는 stem:[\bigtriangledown f]로 표현하고, (stem:[\bigtriangledown](del): 벡터 미분 연산자) +
stem:[f]의 각 성분의 편미분으로 구성된 열 벡터로 정의한다. +
stem:[\bigtriangledown f = (\frac{\partial f}{\partial x_1},...,\frac{\partial f}{\partial x_n})] +
stem:[\bigtriangledown f = (\frac{\partial f}{\partial x},\frac{\partial f}{\partial y})] +
eg> stem:[f(x,y,z) = 2x + 3y^2 - sin(z)] 의 기울기는 +
stem:[\bigtriangledown f = (\frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} + \frac{\partial f}{\partial z}) =(2,6y,-coas(z))]

즉, 경사하강법 공식은 현재의 위치 stem:[x_i]에 학습률 stem:[\eta]에 그 위치에서의 기울기 stem:[\bigtriangledown f(x_i)]만큼을 곱한 값을 뺀만큼 위치를 이동시켜 다음 위치 stem:[x_{i+1}]로 이동한다는 소리다.