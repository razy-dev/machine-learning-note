:rootdir: ..
:chapter: linear_regression
include::{rootdir}/adoc/toc.adoc[]

= Linear Regression Using Autograd
:prev_page: lr_numpy
:next_page: lr_criterion

== Dataset
[source, python]
----
include::{sourcedir}/lr_autograd.py[tag=dataset]
----

include::{rootdir}/adoc/navigation.adoc[]


== Hypothesis(가설)
====
stem:[\hat{y} = wx + b]
====

.Hypothesis Function
[source, python]
----
include::{sourcedir}/lr_autograd.py[tag=hypothesis]
----

include::{rootdir}/adoc/navigation.adoc[]


== Loss(Error) & Cost
Mean Squared Error(MSE: 평균제곱오차)
====
stem:[
\begin{align}
loss &= y_i - \hat{y_i} \\
cost &= \frac 1 n \sum_{i}^{n} loss^2 \\
     &= \frac 1 n \sum_{i}^{n} (y_i - \hat{y_i})^2 \\
     &= \frac 1 n \sum_{i}^{n} (y_i - (wx_i + b))^2 \\
\end{align}
]
====
.Cost Function
[source, python]
----
include::{sourcedir}/lr_autograd.py[tag=cost]
----

include::{rootdir}/adoc/navigation.adoc[]


== Training

=== Gradient Descent Algorithm(기울기 감소 알고리즘)
====
stem:[w := w - lr\frac \partial {\partial w} cost(w, b)]
====
====
stem:[b := b - lr\frac \partial {\partial b} cost(w, b)]
====
[source, python]
----
    cost.backward()  # Autograd
----

=== Optimize
[source, python]
----
    cost.backward()  # Autograd
    with torch.no_grad():
        w -= lr * w.grad
        b -= lr * b.grad
        w.grad = None  # grad 초기화
        b.grad = None  # grad 초기화
----
* lr: Learning Rate

=== Training
[source, python]
----
include::{sourcedir}/lr_autograd.py[tag=training]
----
* lr: Learning Rate

include::{rootdir}/adoc/navigation.adoc[]


== Test
[source, python]
----
include::{sourcedir}/lr_autograd.py[tag=test]
----

include::{rootdir}/adoc/navigation.adoc[]