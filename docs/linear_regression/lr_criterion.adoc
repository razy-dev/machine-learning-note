:rootdir: ..
:chapter: linear_regression
include::{rootdir}/adoc/toc.adoc[]

= Linear Regression Using Loss Function
:prev_page: lr_autograd
:next_page: lr_optimizer

== Dataset
[source, python]
----
include::{sourcedir}/lr_criterion.py[tag=dataset]
----

include::{rootdir}/adoc/navigation.adoc[]


== Hypothesis(가설)
====
stem:[\hat{y} = wx + b]
====

.Hypothesis Function
[source, python]
----
include::{sourcedir}/lr_criterion.py[tag=hypothesis]
----

include::{rootdir}/adoc/navigation.adoc[]


== Loss(Error) & Cost
Mean Squared Error(MSE: 평균제곱오차)

.Cost Function
[source, python]
----
include::{sourcedir}/lr_criterion.py[tag=cost]
----

include::{rootdir}/adoc/navigation.adoc[]


== Training

=== Gradient Descent Algorithm(기울기 감소 알고리즘)
====
stem:[w := w - lr\frac \partial {\partial w} cost(w, b)]
====
====
stem:[b := b - lr\frac \partial {\partial b} cost(w, b)]
====
[source, python]
----
    cost.backward()  # Autograd
----

=== Optimize
[source, python]
----
    with torch.no_grad():
        w -= lr * w.grad
        b -= lr * b.grad
        w.grad = None  # grad 초기화
        b.grad = None  # grad 초기화
----
* lr: Learning Rate

=== Training
[source, python]
----
include::{sourcedir}/lr_criterion.py[tag=training]
----
* lr: Learning Rate

include::{rootdir}/adoc/navigation.adoc[]


== Test
[source, python]
----
include::{sourcedir}/lr_criterion.py[tag=test]
----

include::{rootdir}/adoc/navigation.adoc[]