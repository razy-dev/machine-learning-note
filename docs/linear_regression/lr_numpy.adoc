:rootdir: ..
:chapter: linear_regression
include::{rootdir}/adoc/toc.adoc[]

= Linear Regression
:next_page: lr_autograd

[sidebar]
----
주어진 `x` 와 `y` 의 관계를 제일 잘 설명한 하나의 선형(직선)함수 찾는 것
----

== Dataset
[source, python]
----
include::{sourcedir}/lr_numpy.py[tag=dataset]
----

include::{rootdir}/adoc/navigation.adoc[]


== Hypothesis(가설)
====
stem:[\hat{y} = wx + b]
====

.Hypothesis Function
[source, python]
----
include::{sourcedir}/lr_numpy.py[tag=hypothesis]
----

include::{rootdir}/adoc/navigation.adoc[]


== Loss(Error) & Cost
Mean Squared Error(MSE: 평균제곱오차)
====
stem:[
\begin{align}
loss &= y_i - \hat{y_i} \\
cost &= \frac 1 n \sum_{i}^{n} loss^2 \\
     &= \frac 1 n \sum_{i}^{n} (y_i - \hat{y_i})^2 \\
     &= \frac 1 n \sum_{i}^{n} (y_i - (wx_i + b))^2 \\
\end{align}
]
====

.Cost Function
[source, python]
----
include::{sourcedir}/lr_numpy.py[tag=cost]
----

include::{rootdir}/adoc/navigation.adoc[]


== Trainig

=== Gradient Descent Algorithm(기울기 감소 알고리즘)
====
stem:[
\begin{align}
    \frac \partial {\partial w} cost(w, b)
    &= \frac \partial {\partial w} (\frac{1}{n} \sum_{i=1}^{n} (y_i - (wx_i + b))^2 ) \\
    &= \frac \partial {\partial w} (\frac{1}{n} \sum_{i=1}^{n} (y_i^2 - 2y_i(wx_i + b) +(wx_i + b)^2)) \\
    &= \frac \partial {\partial w} (\frac{1}{n} \sum_{i=1}^{n} (y_i^2 - 2wx_iy_i - 2by_i + w^2x_i^2 + 2wbx_i + b^2)) \\
    \\
    &= \frac{1}{n}  \sum_{i=1}^{n} (-2x_iy_i + 2wx_i^2 + 2bx_i) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} x_i(y_i - wx_i - b) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} x_i(y_i -(wx_i + b)) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} x_i(y_i -\hat{y_i}) \\
\end{align}
]
====
[source, python]
----
include::{sourcedir}/lr_numpy.py[tag=dw]
----

====
stem:[
\begin{align}
    \frac \partial {\partial b} cost(w, b)
    &= \frac \partial {\partial b} (\frac{1}{n} \sum_{i=1}^{n} (y_i - (wx_i + b))^2 ) \\
    &= \frac \partial {\partial b} (\frac{1}{n} \sum_{i=1}^{n} (y_i^2 - 2y_i(wx_i + b) +(wx_i + b)^2)) \\
    &= \frac \partial {\partial b} (\frac{1}{n} \sum_{i=1}^{n} (y_i^2 - 2wx_iy_i - 2by_i + w^2x_i^2 + 2wbx_i + b^2)) \\
    \\
    &= \frac{1}{n}  \sum_{i=1}^{n} (-2y_i +2wx_i +2b) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} (y_i - wx_i - b) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} (y_i - (wx_i + b)) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} (y_i -\hat{y_i}) \\
\end{align}
]
====
[source, python]
----
include::{sourcedir}/lr_numpy.py[tag=db]
----

=== Optimize
====
stem:[
\begin{align}
    w &:= w - lr\frac \partial {\partial w} cost(w, b) \\
    &= w - lr(-\frac{2}{n} \sum_{i=1}^{n} x_i(y_i -\hat{y_i})) \\
    &= w + lr(\frac{2}{n} \sum_{i=1}^{n} x_i(y_i -\hat{y_i})) \\
\end{align}
]
====
====
stem:[
\begin{align}
    b &:= b - lr\frac \partial {\partial b} cost(w, b) \\
    &= b - lr(-\frac{2}{n} \sum_{i=1}^{n} (y_i -\hat{y_i})) \\
    &= b + lr(\frac{2}{n} \sum_{i=1}^{n} (y_i -\hat{y_i})) \\
\end{align}
]
====
* lr: Learning Rate

.Training
[source, python]
----
include::{sourcedir}/lr_numpy.py[tag=training]
----

include::{rootdir}/adoc/navigation.adoc[]


== Test
.Model Test
[source, python]
----
include::{sourcedir}/lr_numpy.py[tag=test]
----

include::{rootdir}/adoc/navigation.adoc[]