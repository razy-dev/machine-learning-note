:rootdir: ..
:chapter: logistic_regression
include::{rootdir}/adoc/toc.adoc[]

= Binary Logistic Regression
:next_page: lg_autograd

[sidebar]
----
주어진 값(들) 또는 그 연산 결과가 특정 범위에 속할 확률(분류)을 찾는 것
----

== Dataset
```python
include::{sourcedir}/lg_numpy.py[tag=dataset]
```

include::{rootdir}/adoc/navigation.adoc[]

== Hypothesis(가설)
=== Linear function
====
stem:[g(x) = z = w_0x_0 + w_1x_1 + \cdots w_ix_i + b]
====

=== Activation function
====
stem:[\sigma(z) = \frac 1 {1 + e^{-z}} = \frac {e^z} {e^z + 1}]
====

=== Hypothesis
====
stem:[
\begin{align}

    H(x) = \hat{y} &= \sigma(z) \\
    &= \frac 1 {1 + e^{-z}} \\
    &= \frac 1 {1 + e^{-(w_0x_0 + w_1x_1 + b)}}
\end{align}
]
====

.Hypothesis Function
```python
include::{sourcedir}/lg_numpy.py[tag=hypothesis]
```

include::{rootdir}/adoc/navigation.adoc[]

== Loss(Error) & Cost
Mean Squared Error(MSE: 평균제곱오차)
====
stem:[
\begin{align}
loss &= y_i - \hat{y_i} \\
cost &= \frac 1 n \sum_{i}^{n} loss^2 \\
     &= \frac 1 n \sum_{i}^{n} (y_i - \hat{y_i})^2 \\
     &= \frac 1 n \sum_{i}^{n} (y_i - (wx_i + b))^2 \\
\end{align}
]
====
.Cost Function
```python
include::{sourcedir}/lg_numpy.py[tag=cost]
```

include::{rootdir}/adoc/navigation.adoc[]

== Trainig

=== Gradient Descent Algorithm(기울기 감소 알고리즘)
====
stem:[
\begin{align}
    \frac \partial {\partial w} cost(w, b)
    &= \frac \partial {\partial w} (\frac{1}{n} \sum_{i=1}^{n} (y_i - (wx_i + b))^2 ) \\
    &= \frac \partial {\partial w} (\frac{1}{n} \sum_{i=1}^{n} (y_i^2 - 2y_i(wx_i + b) +(wx_i + b)^2)) \\
    &= \frac \partial {\partial w} (\frac{1}{n} \sum_{i=1}^{n} (y_i^2 - 2wx_iy_i - 2by_i + w^2x_i^2 + 2wbx_i + b^2)) \\
    \\
    &= \frac{1}{n}  \sum_{i=1}^{n} (-2x_iy_i + 2wx_i^2 + 2bx_i) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} x_i(y_i - wx_i - b) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} x_i(y_i -(wx_i + b)) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} x_i(y_i -\hat{y_i}) \\
\end{align}
]
====
```python
include::{sourcedir}/lg_numpy.py[tag=dw]
```

====
stem:[
\begin{align}
    \frac \partial {\partial b} cost(w, b)
    &= \frac \partial {\partial b} (\frac{1}{n} \sum_{i=1}^{n} (y_i - (wx_i + b))^2 ) \\
    &= \frac \partial {\partial b} (\frac{1}{n} \sum_{i=1}^{n} (y_i^2 - 2y_i(wx_i + b) +(wx_i + b)^2)) \\
    &= \frac \partial {\partial b} (\frac{1}{n} \sum_{i=1}^{n} (y_i^2 - 2wx_iy_i - 2by_i + w^2x_i^2 + 2wbx_i + b^2)) \\
    \\
    &= \frac{1}{n}  \sum_{i=1}^{n} (-2y_i +2wx_i +2b) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} (y_i - wx_i - b) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} (y_i - (wx_i + b)) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} (y_i -\hat{y_i}) \\
\end{align}
]
====
```python
include::{sourcedir}/lg_numpy.py[tag=db]
```

=== Optimize
====
stem:[
\begin{align}
    w &:= w - lr\frac \partial {\partial w} cost(w, b) \\
    &= w - lr(-\frac{2}{n} \sum_{i=1}^{n} x_i(y_i -\hat{y_i})) \\
    &= w + lr(\frac{2}{n} \sum_{i=1}^{n} x_i(y_i -\hat{y_i})) \\
\end{align}
]
====
====
stem:[
\begin{align}
    b &:= b - lr\frac \partial {\partial b} cost(w, b) \\
    &= b - lr(-\frac{2}{n} \sum_{i=1}^{n} (y_i -\hat{y_i})) \\
    &= b + lr(\frac{2}{n} \sum_{i=1}^{n} (y_i -\hat{y_i})) \\
\end{align}
]
====
* lr: Learning Rate
```python
include::{sourcedir}/lg_numpy.py[tag=training]
```

include::{rootdir}/adoc/navigation.adoc[]


== Test
```python
include::{sourcedir}/lg_numpy.py[tag=test]
```

include::{rootdir}/adoc/navigation.adoc[]