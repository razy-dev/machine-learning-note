:rootdir: ..
:chapter: logistic_regression
include::{rootdir}/adoc/toc.adoc[]

= Basic Logistic Regression
:next_page: lg_autograd

[sidebar]
----
주어진 `x` 와 `y` 의 관계를 제일 잘 설명한 하나의 선형(직선)함수 찾는 것
----

== Dataset
```python
include::{sourcedir}/lg_numpy.py[tag=dataset]
```

include::{rootdir}/adoc/navigation.adoc[]

== Hypothesis(가설)
====
stem:[\hat{y} = w_0x_0 + w_1x_1 + \cdots + w_ix_i + b]
====

.Hypothesis Function
```python
include::{sourcedir}/lg_numpy.py[tag=hypothesis]
```

include::{rootdir}/adoc/navigation.adoc[]

== Loss(Error) & Cost
Mean Squared Error(MSE: 평균제곱오차)
====
stem:[
\begin{align}
loss &= y_i - \hat{y_i} \\
cost &= \frac 1 n \sum_{i}^{n} loss^2 \\
     &= \frac 1 n \sum_{i}^{n} (y_i - \hat{y_i})^2 \\
     &= \frac 1 n \sum_{i}^{n} (y_i - (wx_i + b))^2 \\
\end{align}
]
====
.Cost Function
```python
include::{sourcedir}/lg_numpy.py[tag=cost]
```

include::{rootdir}/adoc/navigation.adoc[]

== Trainig

=== Gradient Descent Algorithm(기울기 감소 알고리즘)
====
stem:[
\begin{align}
    \frac \partial {\partial w} cost(w, b)
    &= \frac \partial {\partial w} (\frac{1}{n} \sum_{i=1}^{n} (y_i - (wx_i + b))^2 ) \\
    &= \frac \partial {\partial w} (\frac{1}{n} \sum_{i=1}^{n} (y_i^2 - 2y_i(wx_i + b) +(wx_i + b)^2)) \\
    &= \frac \partial {\partial w} (\frac{1}{n} \sum_{i=1}^{n} (y_i^2 - 2wx_iy_i - 2by_i + w^2x_i^2 + 2wbx_i + b^2)) \\
    \\
    &= \frac{1}{n}  \sum_{i=1}^{n} (-2x_iy_i + 2wx_i^2 + 2bx_i) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} x_i(y_i - wx_i - b) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} x_i(y_i -(wx_i + b)) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} x_i(y_i -\hat{y_i}) \\
\end{align}
]
====
```python
include::{sourcedir}/lg_numpy.py[tag=dw]
```

====
stem:[
\begin{align}
    \frac \partial {\partial b} cost(w, b)
    &= \frac \partial {\partial b} (\frac{1}{n} \sum_{i=1}^{n} (y_i - (wx_i + b))^2 ) \\
    &= \frac \partial {\partial b} (\frac{1}{n} \sum_{i=1}^{n} (y_i^2 - 2y_i(wx_i + b) +(wx_i + b)^2)) \\
    &= \frac \partial {\partial b} (\frac{1}{n} \sum_{i=1}^{n} (y_i^2 - 2wx_iy_i - 2by_i + w^2x_i^2 + 2wbx_i + b^2)) \\
    \\
    &= \frac{1}{n}  \sum_{i=1}^{n} (-2y_i +2wx_i +2b) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} (y_i - wx_i - b) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} (y_i - (wx_i + b)) \\
    &= -\frac{2}{n} \sum_{i=1}^{n} (y_i -\hat{y_i}) \\
\end{align}
]
====
```python
include::{sourcedir}/lg_numpy.py[tag=db]
```

=== Optimize
====
stem:[
\begin{align}
    w &:= w - lr\frac \partial {\partial w} cost(w, b) \\
    &= w - lr(-\frac{2}{n} \sum_{i=1}^{n} x_i(y_i -\hat{y_i})) \\
    &= w + lr(\frac{2}{n} \sum_{i=1}^{n} x_i(y_i -\hat{y_i})) \\
\end{align}
]
====
====
stem:[
\begin{align}
    b &:= b - lr\frac \partial {\partial b} cost(w, b) \\
    &= b - lr(-\frac{2}{n} \sum_{i=1}^{n} (y_i -\hat{y_i})) \\
    &= b + lr(\frac{2}{n} \sum_{i=1}^{n} (y_i -\hat{y_i})) \\
\end{align}
]
====
* lr: Learning Rate
```python
include::{sourcedir}/lg_numpy.py[tag=training]
```

include::{rootdir}/adoc/navigation.adoc[]


== Test
```python
include::{sourcedir}/lg_numpy.py[tag=test]
```

include::{rootdir}/adoc/navigation.adoc[]