include::adoc/_toc.adoc[]

= Artificial Neural Network

== 다중-단층 페셉트론

//@formatter:off

stem:[x = \begin{pmatrix}
x_1 & x_2 & x_3
\end{pmatrix}]

stem:[w = \begin{pmatrix}
w_{11} & w_{21} \\
w_{12} & w_{22} \\
w_{13} & w_{23} \\
\end{pmatrix}]

stem:[b = \begin{pmatrix}
b_1 & b_2
\end{pmatrix}]

stem:[y = x \times w + b]
stem:[\;\;\; = \begin{pmatrix}
x_1 \cdot w_{11} + x_2 \cdot w_{12} + x_3 \cdot w_{13} + b_1
&
x_1 \cdot w_{21} + x_2 \cdot w_{22} + x_3 \cdot w_{23} + b_2
\end{pmatrix}]
//@formatter:on

== 다층 퍼셉트론(MLP:Multi Layer Perceptron)

== 순전파

== 역전파

== ANN

손실 함수(Loss Function)::
최적화(Optimizer)::
역전파(Back Propagation)::

//==

== 손실 함수(Loss Function)

=== 제곱오차(Square Error, SE)

stem:[SE = (y - \hat{y})^2]

=== 오차제곱합(Sum of Squares for Error, SSE)

기본식::
stem:[SSE = \sum_{k}(y_k - \hat{y_k})^2]

신경망에서는::
stem:[E = \frac{1}{2}\sum_{k}(y_k - \hat{y_k})^2 ] +
+
오류를 최소화시키기 위해 stem:[\frac{1}{2}] 을 곱해준다.

=== 평균제곱오차(Mean Square Error, MSE)

stem:[\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2]

=== 평균제곱근오차(Root Mean Square Error, RMSE)

stem:[RMSE = \sqrt{\frac{1}{n}\sum_{k}^{n}(y_k - \hat{y_k})^2}]

=== 교차 엔트로피 오차(Cross Entropy Error, CEE)

=== 이진 교차 엔트로피 오차(Binary Cross Entropy Error)

=== 범주형 교차 엔트로피 오차(Categorical Cross Entropy Error, CCEE)

== 최적화(Optimizer)

stem:[SE = (y - \hat{y})^2]

stem:[\hat{y} = wx + b]

stem:[(y - \hat{y})^2] +
stem:[= (y - (wx + b))^2] +
stem:[= y^2 -2y(wx + b) + (wx + b)^2] +
stem:[= w^2x^2 + 2wxb + b^2 - 2wxy - 2yb + y^2]

=== 경사하강법(Gradient Descent)

stem:[x_{i+1} = x_i - \eta\bigtriangledown f(x_i)]::
stem:[\eta]:::: 학습률(Learning Rate)이라 하며, 한 번의 학습에서 얼마나 이동할지를 정한다.

stem:[\bigtriangledown f(x_i)]::::
스칼라 함수 stem:[f(x)]의 기울기는 stem:[\bigtriangledown f]로 표현하고, (stem:[\bigtriangledown](del): 벡터 미분 연산자) +
stem:[f]의 각 성분의 편미분으로 구성된 열 벡터로 정의한다. +
stem:[\bigtriangledown f = (\frac{\partial f}{\partial x_1},...,\frac{\partial f}{\partial x_n})] +
stem:[\bigtriangledown f = (\frac{\partial f}{\partial x},\frac{\partial f}{\partial y})] +
eg> stem:[f(x,y,z) = 2x + 3y^2 - sin(z)] 의 기울기는 +
stem:[\bigtriangledown f = (\frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} + \frac{\partial f}{\partial z}) =(2,6y,-coas(z))]

즉, 경사하강법 공식은 현재의 위치 stem:[x_i]에 학습률 stem:[\eta]에 그 위치에서의 기울기 stem:[\bigtriangledown f(x_i)]만큼을 곱한 값을 뺀만큼 위치를 이동시켜 다음 위치 stem:[x_{i+1}]로 이동한다는 소리다.